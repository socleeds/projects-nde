# NDE10

<table>
<tr>
<th align="left">For Session</th>
<td>2019/20</td>
</tr>
<tr>
<th align="left">Theme of Projects</th>
<td>Plagiarism in Programming Assignments</td>
</tr>
<tr>
<th align="left">Supervisor</th>
<td>Nick Efford</td>
</tr>
<tr>
<th align="left">Project Type</th>
<td>Software Product / Exploratory Software / Empirical Investigation</td>
</tr>
<tr>
<th align="left">Areas of Interest</th>
<td>Software engineering, programming</td>
</tr>
<tr>
<th align="left">Appropriate For</th>
<td>All programmes</td>
</tr>
<tr>
<th align="left">Multiple Projects?</th>
<td>Yes</td>
</tr>
<tr>
<th align="left">Prerequisites</th>
<td>Good programming skills</td>
</tr>
</table>

## Problem Domain

Like other institutions, the University of Leeds considers plagiarism a
serious matter.  The University subscribes to, and encourages the use of,
standard similarity detection tools such as [Turnitin](https://turnitin.com).
Turnitin is effective at flagging similarities in essays and dissertations,
but is not designed to work with program source code.  Staff in the School
of Computing have been using Stanford University's
[Moss](https://theory.stanford.edu/~aiken/moss/) as a tool for software
similarity detection, but this is not entirely satisfactory: it relies on
an external server, the availability of which cannot be guaranteed; a lot of
effort must be expended to prepare code for submission to this server; and the
results of detection can be hard to interpret, and can't easily be captured
for later review.

Beyond the technical challenges of detecting similarity, there are other
questions to consider.  Is plagiarism of software different from other forms
of plagiarism?  Can the advice we offer to students be improved to reduce
instances of software plagiarism?  Can we redesign our approaches to
assessment to reduce the opportunities or temptations to plagiarise?

## Possible Projects

There could be multiple projects looking at different aspects of this problem,
spanning the Software Product (SP), Exploratory Software (ES) or Empirical
Investigation (EI) categories.  Here are some possibilities:

* Improve the UI for Moss, developing easier ways of submitting coursework
  to it and capturing/visualising the results (SP)

* Develop a new similarity detection system that runs locally and is tailored
  to our needs, integrating with other systems (Minerva, GitLab, etc) (SP)

* Investigate the state of the art in software similarity detection and
  develop prototypes that explore and evaluate different approaches (ES)

* Investigate students' understanding of plagiarism as it applies to software,
  look at what happens in other universities, draw on this research to
  explore ways in which guidance issued to students can be improved,
  or ways in which assignments can be redesigned to prevent or discourage
  plagiarism (EI)
